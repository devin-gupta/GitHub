{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 9\n",
    "#### 5/2 Ramesh Johari's Introduction to Game Theory\n",
    "\n",
    "Exam Notes: \n",
    "- collection of multiple choice questions\n",
    "- 1 longer form pset type question\n",
    "\n",
    "Repeated games\n",
    "- class of dynamic games were we repeat the same static game of complete information.\n",
    "- independent field of study on reputation games in long-run games. \n",
    "\n",
    "The example of prisoner's dilemna\n",
    "1. At stage <i>K</i>, no stages left, there is no future and hence we shouldn't cooperate and both players will defect, regardless of the history up to this point. \n",
    "2. Now at stage <i>K</i> -1, both players know the next stage will be both defecting, regardless of the outcome at this stage and regardless of history.\n",
    "3. And hence at stage <i>K</i> -1, both players will treat it like this is the last move which means both people will defect.\n",
    "4. And so on...\n",
    "\n",
    "<b>\"strategy\"</b> (in repeated games) : after every possible history, at every stage, what is the action of every player. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Tit-for-that\" strategy. In 1980 Robert Axelrod held a computer tournament to compare submitted strategies for the prisoner's dilemma. \n",
    "\n",
    "The winner was the TfT strategy: \n",
    "- cooperate at the first stage\n",
    "- at every subsequent action: play the same action as your opponent in the previous round. Even though both players playing TfT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['4,4' '1,5' '-1,0']\n",
      " ['5,1' '3,3' '0,0']\n",
      " ['0,-1' '0,0' '1,1']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = [['4,4', \"1,5\", \"-1,0\"],\n",
    "    [\"5,1\", \"3,3\", \"0,0\"],\n",
    "    [\"0,-1\", \"0,0\", \"1,1\"]]\n",
    "\n",
    "print(np.matrix(A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeated games with infinite horizon:\n",
    "\n",
    "We use discounted payoffs to model these games because we don't want infinite payoffs. \n",
    "\n",
    "Each player <i>i</i> discounts future payoffs by a <b>discount factor </b> $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PunishDefection:\n",
    "    if a1 == \"4,4\": \n",
    "        print(\"B\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
