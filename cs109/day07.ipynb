{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 7\n",
    "#### 8/8 CS 109 Binomial, Bernoulli and Variance\n",
    "\n",
    "https://web.stanford.edu/class/archive/cs/cs109/cs109.1238/lectures/7-BernoulliBinomial/7-BernoulliBinomial.pdf\n",
    "\n",
    "Code below: Bernoulli calculates the probability of a random variable with a variable of 0.25. \n",
    "\n",
    "* Expected value loops over all the possible x values, multiplying the value by the probability of that value\n",
    "$$ E[X] = \\sum_{x_i}^{} x_i * P(X=x_i) $$\n",
    "* Properties of Expectation\n",
    "    * Linearity: \n",
    "    * Expecation of a sum\n",
    "    * Unconcious Statistician\n",
    "\n",
    "1. Bernoulli Random Variables\n",
    "    * Often you want to evaluate in binary. this is what bernoulli came up with. \n",
    "    * \"indicator random variable\"\n",
    "2. Binomial\n",
    "    * sum of $n$ bernoulli random variables\n",
    "\n",
    "Large Applicability: Useful for any problem where there are $n$ independent trials, each with prob $p$ of success, and need to solve prob of $k$ successes.\n",
    "\n",
    "$X$ ~ Bin($n,p$) means Our random Variable Is distributed as a Binomial given the Number of Trials and Probability of success of each trial.\n",
    "Then the PMF is \n",
    "$$  P(X=k) = { n \\choose k } * p^k * (1-p)e(n-p)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bernoulli(object):\n",
    "    def __init__(self, parameter):\n",
    "        self.p = parameter\n",
    "        self.pmf = {1: parameter, 0: 1 - parameter}\n",
    "    \n",
    "    def sample(self):\n",
    "        rand = random.uniform(0, 1)\n",
    "        if rand < self.p:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "# P(X = x)\n",
    "def Probability(X, x):\n",
    "    return X.pmf[x]\n",
    "\n",
    "#  E[X]\n",
    "def Expectation(X):\n",
    "    ev = 0\n",
    "    for x, px in X.pmf.items():\n",
    "        ev += x * px\n",
    "    return ev\n",
    "\n",
    "def main():\n",
    "    # X ~ Bern(0.25)\n",
    "    X = Bernoulli(0.25)\n",
    "\n",
    "    # P(X = 1)\n",
    "    probability = Probability(X, 1)\n",
    "    print(\"P(X = 1) = \", probability)\n",
    "\n",
    "    # E[X]\n",
    "    expected_value = Expectation(X)\n",
    "    print(\"E[X] = \", expected_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider binomial distribution questions like these: \n",
    "\n",
    "If we serve 1000 ads, each with a p = 0.01, what is the probability of exactly 5 clicks?\n",
    "OR warriors playing bucks, warrriors win 55% and each game is independent. win at least 4 of 7 games to win series. prob of winning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob of 5 clicks:  0.03745311160824718\n",
      "prob of winning series:  0.6082877968750001\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(\"prob of 5 clicks: \", stats.binom.pmf(5, 1000, 0.01))\n",
    "print(\"prob of winning series: \", stats.binom.pmf(4, 7, 0.55) + stats.binom.pmf(5, 7, 0.55) + stats.binom.pmf(6, 7, 0.55) + stats.binom.pmf(7, 7, 0.55))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure spared as a measure of the \"distribution\" of things with an equal expectation.\n",
    "Variance is the expectation of the square of the distance of every value from the mean. STD is the square root of the variance.\n",
    "$$ Var(X) = E[(X-\\mu)^2]$$\n",
    "Normalized histograms are approximations of the probability density function (PDF) of the data.\n",
    "Through a pretty cool derivation, we derive:\n",
    "$$ Var(X) = E[(X-\\mu)^2] = \\sum_{x} (x-\\mu)^2 * p(x) = E[X^2] - (E[X])^2 $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
