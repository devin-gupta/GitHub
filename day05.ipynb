{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 5, Reinforcement Learning\n",
    "\n",
    "We'll learn today how to formulate and iterate on an RL method. \n",
    "\n",
    "#### What is RL? \n",
    "- In each example of RL, we have a reward from an environment and the agent/policy gets sate of world.\n",
    "- Markov Decision Process is built $\\rightarrow$ RL tries to find agent that maximizes reward of actions in game.\n",
    "\n",
    "#### RL for Pupper\n",
    "- Environment $=$ the physical world where Pupper lives, including Pupper itself\n",
    "- Observation $=$ where the end affector of the feet are, or the joint angles of the motors\n",
    "- Actions $=$ any changes to the joint angles. delta to joint angles. Can also use cartesian coordinates to tell end effector positions\n",
    "- Rewards $=$ distance it walks before it falls, speed it walks at\n",
    "- Termination point $=$ point we stop possibly after some time, when it hits ground and falls. \n",
    "\n",
    "#### Solving RL Problem\n",
    "- Maximation problem $\\rightarrow$ maximize reward given policy/agent. \n",
    "\n",
    "$$ \\text{maximize}_{\\pi_{\\theta} (s)} \\sum_{t=0}^{T} r(s_t, a_t) $$\n",
    "\n",
    "To Solve, follow these steps:\n",
    "1. Intiialize $\\theta_{0}$\n",
    "2. Compute Gradient $g = \\frac{\\delta}{}$\n",
    "3. Update Agent\n",
    "4. Repeat 1-3\n",
    "\n",
    "Gradient is not easy to compute because it requires the reward to be differntiable. This is related to modeling reward using neural network. We also would need the system dynamics to be differentiable hence is challenging to do. \n",
    "\n",
    "#### Stochastic Policy to the Rescue\n",
    "\n",
    "Deterministic Policy versus stochastic policies. We're getting the expectation of reward across the probablistic distribution of policies. \n",
    "\n",
    "Some new technqiues to solve RL problems: \n",
    "- Generalized Advantage Estimator (GAE)\n",
    "- PPO or TRPO helps reuse samples from old policies. \n",
    "\n",
    "#### Sim2Real\n",
    "- Use Domain Randomization which helps generalizability of DRL algos.\n",
    "- New simulations can train 750 days of sim days within 1 real day. Before we'd train 0.5 sim days in 3 real hours. Proportion is becoming way better.\n",
    "- GPU based training is 500x faster than CPU speed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
